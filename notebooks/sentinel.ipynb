{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb26ba86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\PrinciaFernandes\\\\Mresult\\\\Phenomix\\\\notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2025e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d2ceed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\PrinciaFernandes\\\\Mresult\\\\Phenomix'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fe0c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber\n",
    "import io\n",
    "import json\n",
    "from models.llm_model.gemini_model import json_response\n",
    "from models.prompts.Prompts import prompt_overview,prompt_description\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e02f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def overview_extract(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        overview_page = ''\n",
    "        for page in pdf.pages[:4]:\n",
    "            text = page.extract_text()\n",
    "            if 'Overview' in text:\n",
    "                overview_page += text    \n",
    "            if ' Request Send Date' in text and  'Glossary' not in text: \n",
    "                overview_page += text   \n",
    "        overview = json_response(prompt_overview(overview_page))\n",
    "    return overview\n",
    "\n",
    "def description_extract(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        description_dict = []\n",
    "        chunk_size = 3\n",
    "        pages = len(pdf.pages)\n",
    "        chunks = [range(i,i+chunk_size) if i + chunk_size <= pages else range(i,pages) for i in range(3,pages,chunk_size) ]\n",
    "        for chunk in chunks:\n",
    "            extracted = ''\n",
    "            extracted_data = []\n",
    "            for each_page in chunk:\n",
    "                page = pdf.pages[each_page]\n",
    "                text = page.extract_text()\n",
    "                if not 'Glossary' in text and not 'Generic Name' in text and not 'Brand Name' in text:\n",
    "                    if len((extracted + text).split()) < 1800:\n",
    "                        extracted += text\n",
    "                    else:\n",
    "                        extracted_data.append(text)\n",
    "            if extracted != '':\n",
    "                extracted_data.insert(0,extracted)\n",
    "            for text in extracted_data:\n",
    "                description = json_response(prompt_description(text))\n",
    "                description_new = json.loads(description.replace(\"\\n\",''))\n",
    "                if len(description_new) > 1:\n",
    "                    phenotype_description = []\n",
    "                    codes = description_new\n",
    "                    columns = codes[0].split(\"|\")\n",
    "                    for row in codes[1:]:\n",
    "                        rows = row.split(\"|\")\n",
    "                        d ={}\n",
    "                        for k,v in zip(columns,rows):\n",
    "                            d[k] = v\n",
    "                        phenotype_description.append(d)\n",
    "                    description_dict += phenotype_description\n",
    "                else:\n",
    "                    continue\n",
    "    return description_dict\n",
    "\n",
    "def extract_exceptionl(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:   \n",
    "        extracted_text = ''\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if 'Overview' in text:\n",
    "                pass  \n",
    "            if ' Request Send Date' in text and  'Glossary' not in text: \n",
    "                pass\n",
    "            else:\n",
    "                extracted_text += text\n",
    "        overview = overview_extract(pdf_path)\n",
    "        \n",
    "        description = json_response(prompt_description(extracted_text))\n",
    "        description_json = json.loads(description.replace(\"\\n\",''))\n",
    "        if len(description_json) > 1:\n",
    "            phenotype = []\n",
    "            codes = description_json\n",
    "            columns = codes[0].split(\"|\")\n",
    "            for row in codes[1:]:\n",
    "                rows = row.split(\"|\")\n",
    "                code_dictionary ={k:v for k,v in zip(columns,rows)}\n",
    "                phenotype.append(code_dictionary)\n",
    "    phenotype_dict = json.loads(overview)\n",
    "    phenotype_dict[\"Code_Description\"] = phenotype\n",
    "    return phenotype_dict\n",
    "\n",
    "def phenotype(pdf_url):\n",
    "    response = requests.get(pdf_url)\n",
    "    pdf_content = response.content\n",
    "    pdf_path = io.BytesIO(pdf_content)\n",
    "\n",
    "    if not 'algorithm_Critical_COVID_updated.pdf' in pdf_path:\n",
    "        overview = overview_extract(pdf_path)\n",
    "        description = description_extract(pdf_path)\n",
    "        phenotype_dict = json.loads(overview)\n",
    "        phenotype_dict['Code_Description'] = description\n",
    "    else:\n",
    "        phenotype_dict = extract_exceptionl(pdf_path)\n",
    "\n",
    "    return phenotype_dict\n",
    "\n",
    "\n",
    "def sentinel_scrapping(base_url):\n",
    "    sentinel  = []\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    page_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if 'health-outcomes-interest/' in link['href']: \n",
    "            full_link = requests.compat.urljoin(base_url, link['href'])\n",
    "            response = requests.get(full_link)\n",
    "            if response.status_code == 200:\n",
    "                soup_page = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                if \"Outcomes Assessed in Inferential Analyses\" in soup_page.get_text():\n",
    "                    page_links.append(full_link)\n",
    "                    \n",
    "    for link in page_links[:2]:\n",
    "        content = requests.get(link)\n",
    "        if response.status_code == 200:  \n",
    "            soup_page = BeautifulSoup(content.text,'html.parser')\n",
    "        for link in soup_page.find_all('a', href=True):\n",
    "            if \".pdf\" in link['href']: \n",
    "                pdf_link = requests.compat.urljoin(base_url, link['href'])\n",
    "                sentinel_dict = phenotype(pdf_link)\n",
    "                sentinel.append(sentinel_dict)\n",
    "    return sentinel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02616b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_json\n",
    "from src.config import RAW_DIR,SENTINEL_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a062783",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    base_url = SENTINEL_URL\n",
    "    sentinel = sentinel_scrapping(base_url)\n",
    "    save_json(RAW_DIR,sentinel,'SENTINEL')\n",
    "    return sentinel\n",
    "\n",
    "sentinel = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff74d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70af5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_query_date(date):\n",
    "    if date:\n",
    "        query_period = date.replace(\" to \",\" - \")\n",
    "        query_period = date.replace(\" â€“ \",\" - \")\n",
    "\n",
    "        query_dates = re.findall(r\"\\b[A-Za-z]+\\s\\d{1,2},\\s\\d{4}\\s-\\s[A-Za-z]+\\s\\d{1,2},\\s\\d{4}|\\b\\d{4}\\s*-\\s*\\d{4}\\b\",query_period)\n",
    "        if any(\" - \" in d for d in query_dates): \n",
    "            start_date = [datetime.strptime(date.split(\" - \")[0],\"%B %d, %Y\") for date in query_dates]\n",
    "            end_date = [datetime.strptime(date.split(\" - \")[-1],\"%B %d, %Y\") for date in query_dates]\n",
    "        else:\n",
    "            start_date = [datetime.strptime(date.split(\"-\")[0],\"%Y\") for date in query_dates]\n",
    "            end_date = [datetime.strptime(date.split(\"-\")[-1],\"%Y\") for date in query_dates]\n",
    "        Query_start_date = \", \".join([datetime.strftime(date,\"%Y-%m-%dT%H:%M:%S.00Z\") for date in start_date])\n",
    "        Query_end_date = \", \".join([datetime.strftime(date,\"%Y-%m-%dT%H:%M:%S.00Z\") for date in end_date])\n",
    "    else:\n",
    "        Query_start_date = 'NA'\n",
    "        Query_end_date = 'NA'\n",
    "    return Query_start_date,Query_end_date\n",
    "\n",
    "def get_request_date(date):\n",
    "    if date:\n",
    "        request_date = re.findall(r\"\\b[A-Za-z]+\\s\\d{1,2},\\s\\d{4}\",date)\n",
    "        Request_date = [datetime.strptime(date,\"%B %d, %Y\") for date in request_date]\n",
    "        Request_send_date = \", \".join([datetime.strftime(date,\"%Y-%m-%dT%H:%M:%S.00Z\") for date in Request_date])\n",
    "    else:\n",
    "        Request_send_date = 'NA'\n",
    "    return Request_send_date\n",
    "\n",
    "def get_detail(sentinel):\n",
    "    outcome_list = []\n",
    "    sentinel_detail = []\n",
    "\n",
    "    for phenotype in sentinel:\n",
    "        if phenotype['Overview']['Outcome'] not in outcome_list:\n",
    "            outcome_list.append(phenotype['Overview']['Outcome'])        \n",
    "            detail_dictionary = {}\n",
    "            detail_dictionary['Outcome'] = phenotype['Overview']['Outcome']\n",
    "            detail_dictionary['Title'] = phenotype['Overview']['Title']\n",
    "            detail_dictionary['Request_id'] = phenotype['Overview']['Request IDs'] if phenotype['Overview']['Request IDs'] else 'NA'\n",
    "            detail_dictionary['Query_start_date'],detail_dictionary['Query_end_date'] = get_query_date(phenotype['Overview']['Query period'])\n",
    "            detail_dictionary['Description'] = phenotype['Overview']['Description']\n",
    "            detail_dictionary['Algorithm_to_define_outcome'] = phenotype['Overview']['Algorithm to define outcome']\n",
    "            detail_dictionary['Request_send_date'] = get_request_date(phenotype['Overview']['Request to send dates'])\n",
    "            sentinel_detail.append(detail_dictionary)\n",
    "        else:\n",
    "            for detail in sentinel_detail:\n",
    "                if detail['Outcome'] == phenotype['Overview']['Outcome']:\n",
    "                    detail['Title'] = detail['Title']+ f' \\n {phenotype['Overview']['Title']}' \n",
    "                    detail['Request_id'] = detail['Request_id']+ f' \\n {phenotype['Overview']['Request IDs']}' \n",
    "                    query_start_date,query_end_date = get_query_date(phenotype['Overview']['Request to send dates'])\n",
    "                    detail['Query_start_date'] = detail['Query_start_date']+ f' \\n {query_start_date}'\n",
    "                    detail['Query_end_date'] = detail['Query_end_date']+ f' \\n {query_end_date}' \n",
    "                    detail['Description'] = detail['Description']+ f' \\n {phenotype['Overview']['Description']}' \n",
    "                    detail['Algorithm_to_define_outcome'] = detail['Algorithm_to_define_outcome']+ f' \\n {phenotype['Overview']['Algorithm to define outcome']}' \n",
    "                    detail['Request_send_date'] = detail['Request_send_date']+ f' \\n {get_request_date(phenotype['Overview']['Request to send dates'])}' \n",
    "        \n",
    "    sorted_detail = sorted(sentinel_detail,key = lambda x: x['Outcome'])\n",
    "\n",
    "    i = 0\n",
    "    for detail in sorted_detail:\n",
    "        i += 1\n",
    "        detail['PID'] = f'SP{i:06d}'\n",
    "\n",
    "    return sorted_detail\n",
    "\n",
    "def get_concept(sentinel,detail):\n",
    "    sentinel_concept = []\n",
    "    code_list = []\n",
    "    for phenotype in sentinel:\n",
    "        for codes in phenotype['Code_Description']: \n",
    "            if codes['Code'] not in code_list:\n",
    "                code_list.append(codes['Code'])\n",
    "                concept_dictioanry = {}\n",
    "                concept_dictioanry['Code'] =  codes['Code']\n",
    "                concept_dictioanry['Description'] = codes['Description'] if codes['Description'] else ['NA']\n",
    "                concept_dictioanry['Care_setting'] = [codes['Care_setting']] if 'Care_setting' in codes.keys() else ['NA']\n",
    "                concept_dictioanry['Code_type']= [codes['Code_Type']] if 'Code_Type' in codes.keys() else ['NA']\n",
    "                concept_dictioanry['Code_category']= [codes['Code_Category']] if 'Code_Category' in codes.keys() else ['NA']\n",
    "                concept_dictioanry['Principal_diagnosis']= [codes['Principal diagnosis']] if 'Principal_diagnosis' in codes.keys() else ['NA']\n",
    "                concept_dictioanry['Outcome']= [phenotype['Overview']['Outcome']]\n",
    "                concept_dictioanry['Request_id']= [phenotype['Overview']['Request IDs']]\n",
    "                concept_dictioanry['PIDs']= [d['PID'] for d in detail if d['Outcome'] == phenotype['Overview']['Outcome']]\n",
    "                sentinel_concept.append(concept_dictioanry)\n",
    "            else:\n",
    "                for concept in sentinel_concept:\n",
    "                    if concept['Code'] == codes['Code']:\n",
    "                        concept['Care_setting'].append(codes['Care_setting']) if 'Care_setting' in codes.keys() else concept['Care_setting'].append('NA')\n",
    "                        concept['Code_type'].append(codes['Code_Type']) if 'Code_Type' in codes.keys() else concept['Code_type'].append('NA')\n",
    "                        concept['Code_category'].append(codes['Code_Category']) if 'Code_Category' in codes.keys() else concept['Code_category'].append('NA')\n",
    "                        concept['Principal_diagnosis'].append(codes['Principal_diagnosis']) if 'Principal_diagnosis' in codes.keys() else concept['Principal_diagnosis'].append('NA')\n",
    "                        concept['Outcome'].append(phenotype['Overview']['Outcome'])\n",
    "                        concept['Request_id'].append(phenotype['Overview']['Request IDs']) if phenotype['Overview']['Request IDs'] else concept['Request_id'].append('NA')\n",
    "                        concept['PIDs'].extend([d['PID'] for d in detail if d['Outcome'] == phenotype['Overview']['Outcome']])\n",
    "\n",
    "    sorted_concept = sorted(sentinel_concept,key = lambda x: x['Code'])\n",
    "    i = 0\n",
    "    for dict in sorted_concept:\n",
    "        i += 1\n",
    "        dict['CID'] = f'SC{i:06d}'\n",
    "\n",
    "    return sorted_concept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d42935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_detail,save_concept\n",
    "from src.config import SENTINEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c69b3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # dir = r'Sentinel/'\n",
    "    # with open(rf'{dir}\\sentinel_phenotypes.json','r') as file:\n",
    "    #     data = file.read()\n",
    "    #     sentinel = json.loads(data)\n",
    "    detail = get_detail(sentinel)\n",
    "    concept = get_concept(sentinel,detail)\n",
    "    save_detail(SENTINEL_DIR,detail,'SENTINEL')\n",
    "    save_concept(SENTINEL_DIR,concept,'SENTINEL')\n",
    "    return detail,concept\n",
    "\n",
    "sentinel_detail,sentinel_concept = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0f6b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_json\n",
    "from src.config import RAW_DIR,SENTINEL_URL\n",
    "from src.utils import save_detail,save_concept\n",
    "from src.config import SENTINEL_DIR\n",
    "\n",
    "from src.scraping.sentinel_webscrapping import sentinel_scrapping\n",
    "\n",
    "from src.processing.sentinel_concept_detail import get_detail, get_concept\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6e2c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SentinalPipeline:\n",
    "\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        self.detail = None\n",
    "\n",
    "    def main(self):\n",
    "        base_url = SENTINEL_URL\n",
    "        sentinel = sentinel_scrapping(base_url)\n",
    "        save_json(RAW_DIR,sentinel,'SENTINEL')\n",
    "        self.detail = get_detail(sentinel)\n",
    "        concept = get_concept(sentinel,self.detail)\n",
    "        save_detail(SENTINEL_DIR,self.detail,'SENTINEL')\n",
    "        save_concept(SENTINEL_DIR,concept,'SENTINEL')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80107688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"gemini\": \"Gemini may refer to several things, most notably Google\\'s Gemini AI model. It could also refer to the constellation Gemini, or other entities that use the name. To provide a more specific definition, please clarify which Gemini you are asking about.\"\\n}'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.llm_model.gemini_model import json_response\n",
    "\n",
    "json_response(\"What is gemini?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
